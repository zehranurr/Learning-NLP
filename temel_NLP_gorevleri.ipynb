{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temel NLP Görevleri\n",
    "- Metin sınıflandırma (Text Classification)(Machine Learning)\n",
    "- Varlık ismi Tanıma (Naemd Entity Recognation (NER))(Spacy Dil Modelleri)\n",
    "- Morfolojik analiz (Morphological Analysis)(Spacy Dil Modelleri)\n",
    "- Metin Parçası Etiketleme(PArt of speech)POS tagging(Spacy dil modeli)\n",
    "- kelime anlamı belirsizliği giderme (Word sence disambiguation)Lesk algoritması\n",
    "- Duygu analizi (Sentiment analysis ) MAchine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metin Sınıflandırma(Text Classification)\n",
    "- Metinlerin otomatik olarak belirli kategorilere ayrılmasını sağlar\n",
    "- E posta filtreleme\n",
    "- Doküman yönetimi \n",
    "- Sosyal medya izleme\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Şema\n",
    "┌─────────────┐\n",
    "│   Raw text  │\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐\n",
    "│ Tokenization│\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐\n",
    "│Text cleaning│\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐\n",
    "│ POS tagging │\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐\n",
    "│ Stopwords   │\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐\n",
    "│Lemmatization│\n",
    "└─────┬───────┘\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────┐    ┌─────────────┐\n",
    "│ Cleaned text│───▶│   ML Model  │\n",
    "└─────────────┘    └─────────────┘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working on spam data set please check -> https://www.kaggle.com/code/zehranurmangal/text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition:\n",
    "- Metin içeriisndeki kişi,yer organizasyon gibi özel isimleri tanımlayarak yapılandırılmamış veriden anlamlı bilgiler çıkarmayı sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bilgi çıkarımı \n",
    "- otomatik özetleme\n",
    "- Müşteri ilişkileri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "content = \"john works at microsoft and lives in new york He visited the National History Museum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john works 0 10 PERSON\n",
      "microsoft 14 23 ORG\n",
      "new york 37 45 GPE\n",
      "the National History Museum 57 84 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(content)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entites =[(ent.text,ent.label_,ent.lemma_) for ent in doc.ents]\n",
    "\n",
    "df = pd.DataFrame(entites,columns=[\"text\",\"type\",\"Lemma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john works</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>john work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>ORG</td>\n",
       "      <td>microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york</td>\n",
       "      <td>GPE</td>\n",
       "      <td>new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the National History Museum</td>\n",
       "      <td>ORG</td>\n",
       "      <td>the National History Museum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text    type                        Lemma\n",
       "0                   john works  PERSON                    john work\n",
       "1                    microsoft     ORG                    microsoft\n",
       "2                     new york     GPE                     new york\n",
       "3  the National History Museum     ORG  the National History Museum"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morfolojik analiz\n",
    "- kelimelerin yapısını inceleyerek dilbilgisel özelliklerini belirler\n",
    "- Kullanım Alanları:\n",
    "    - Dil öğrenme araçları\n",
    "    - Doğal Dil işleme\n",
    "    - Otomatik Çeviri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "word = \"books\"\n",
    "\n",
    "doc = nlp(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:   books\n",
      "Leamma book\n",
      "Pos:  NOUN\n",
      "Tag:  NNS\n",
      "Dependency:  ROOT\n",
      "shape:  xxxx\n",
      "ıs alpha:  True\n",
      "Is stop  False\n",
      "Morphology:  Number=Plur\n",
      "ıs plural: True\n"
     ]
    }
   ],
   "source": [
    "for token in doc : \n",
    "    print(\"Text:  \",token.text)\n",
    "    print(\"Leamma\", token.lemma_)\n",
    "    print(\"Pos: \" ,token.pos_),\n",
    "    print(\"Tag: \", token.tag_)\n",
    "    print(\"Dependency: \",token.dep_)#diğer kelimelere bağlı mı \n",
    "    print(\"shape: \",token.shape_)\n",
    "    print(\"ıs alpha: \",token.is_alpha)#sadece kelimelerden mi oluşuyor onu belirler\n",
    "    print(\"Is stop \", token.is_stop)\n",
    "    print(\"Morphology: \", token.morph)\n",
    "    print(f\"ıs plural: {'Number=Plur' in token.morph}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:   books\n",
      "Leamma book\n",
      "Pos:  NOUN\n",
      "Tag:  NNS\n",
      "Dependency:  ROOT\n",
      "shape:  xxxx\n",
      "ıs alpha:  True\n",
      "Is stop  False\n",
      "Morphology:  Number=Plur\n",
      "ıs plural: True\n",
      " \n",
      "Text:   123\n",
      "Leamma 123\n",
      "Pos:  NUM\n",
      "Tag:  CD\n",
      "Dependency:  nummod\n",
      "shape:  ddd\n",
      "ıs alpha:  False\n",
      "Is stop  False\n",
      "Morphology:  NumType=Card\n",
      "ıs plural: False\n",
      " \n",
      "Text:   the\n",
      "Leamma the\n",
      "Pos:  DET\n",
      "Tag:  DT\n",
      "Dependency:  det\n",
      "shape:  xxx\n",
      "ıs alpha:  True\n",
      "Is stop  True\n",
      "Morphology:  Definite=Def|PronType=Art\n",
      "ıs plural: False\n",
      " \n",
      "Text:   Ankara\n",
      "Leamma Ankara\n",
      "Pos:  PROPN\n",
      "Tag:  NNP\n",
      "Dependency:  npadvmod\n",
      "shape:  Xxxxx\n",
      "ıs alpha:  True\n",
      "Is stop  False\n",
      "Morphology:  Number=Sing\n",
      "ıs plural: False\n",
      " \n"
     ]
    }
   ],
   "source": [
    "word2 = \"books 123 the Ankara\"\n",
    "\n",
    "docs = nlp(word2)\n",
    "\n",
    "\n",
    "for token in docs : \n",
    "    print(\"Text:  \",token.text)\n",
    "    print(\"Leamma\", token.lemma_)\n",
    "    print(\"Pos: \" ,token.pos_),\n",
    "    print(\"Tag: \", token.tag_)\n",
    "    print(\"Dependency: \",token.dep_)#diğer kelimelere bağlı mı \n",
    "    print(\"shape: \",token.shape_)\n",
    "    print(\"ıs alpha: \",token.is_alpha)#sadece kelimelerden mi oluşuyor onu belirler\n",
    "    print(\"Is stop \", token.is_stop)\n",
    "    print(\"Morphology: \", token.morph)\n",
    "    print(f\"ıs plural: {'Number=Plur' in token.morph}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metin parçası etiketleme - Part of speech POS\n",
    "\n",
    "- Bir metindeki her kelimenin dilbilgisel kategorisne belirleyerek cümlenin dil yapısını analiz etmeyi sağlar\n",
    "- Kullanım alanları\n",
    "    - Doğal dil işleme\n",
    "    - Makine çevirisi\n",
    "    - Sözcük Türü Analizi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "sentence1 = \"What is the weather like today\"\n",
    "\n",
    "doc1 = nlp(sentence1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What PRON\n",
      "is AUX\n",
      "the DET\n",
      "weather NOUN\n",
      "like ADP\n",
      "today NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "went VERB\n",
      "to ADP\n",
      "the DET\n",
      "store NOUN\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "they PRON\n",
      "were AUX\n",
      "closed ADJ\n",
      ", PUNCT\n",
      "so ADV\n",
      "I PRON\n",
      "had VERB\n",
      "to PART\n",
      "go VERB\n",
      "to ADP\n",
      "another DET\n",
      "store NOUN\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"I went to the store, but they were closed, so I had to go to another store \"\n",
    "doc2 = nlp(sentence2)\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kelime anlamı belirsizliği giderme -Word sense disambiguation\n",
    "- bir kelimenin farklı anlamları arasından doğru olanı bağlama göre seçme işlemdiir.\n",
    "exp : Çay -> akarsu mu ? içiçek çay mı ?\n",
    "\n",
    "- Kullanım alanları\n",
    "    - Makine çevirisi\n",
    "    - Arama motorlaro\n",
    "    - Doğal dil işleme\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to .venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to .venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anlam ayrıştırma algoritması  LESK\n",
    "import nltk \n",
    "from nltk.wsd import lesk\n",
    "\n",
    "nltk.download(\"wordnet\", download_dir=\".venv\\Lib\\site-packages\\\\nltk\")\n",
    "nltk.download(\"omw-1.4\", download_dir=\".venv\\Lib\\site-packages\\\\nltk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\omw-1.4.zip başarıyla çıkartıldı.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Dosya yolunu belirtin\n",
    "zip_file_path = r\"C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\omw-1.4.zip\"\n",
    "extract_to_path = r\"C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\omw-1.4\"\n",
    "\n",
    "# ZIP dosyasını çıkar\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)\n",
    "\n",
    "print(f\"{zip_file_path} başarıyla çıkartıldı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\wordnet.zip başarıyla çıkartıldı.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Dosya yolunu belirtin\n",
    "zip_file_path = r\"C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\wordnet.zip\"\n",
    "extract_to_path = r\"C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\wordnet\"\n",
    "\n",
    "# ZIP dosyasını çıkar\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)\n",
    "\n",
    "print(f\"{zip_file_path} başarıyla çıkartıldı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  I went to bank to deposit money\n",
      "Word:   bank\n",
      "Sense:  a container (usually with a slot in the top) for keeping money at home\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I went to bank to deposit money\"\n",
    "\n",
    "word1 =\"bank\"\n",
    "sense1 = lesk(nltk.word_tokenize(sentence1),word1)\n",
    "\n",
    "print(\"Sentence: \", sentence1)\n",
    "print(\"Word:  \", word1)\n",
    "print(\"Sense: \" ,sense1.definition())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The river bank was flooded after the heavy rain\n",
      "Word:   bank\n",
      "Sense:  a container (usually with a slot in the top) for keeping money at home\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"The river bank was flooded after the heavy rain\"\n",
    "word2 =\"bank\"\n",
    "sense2 = lesk(nltk.word_tokenize(sentence2),word2)\n",
    "\n",
    "print(\"Sentence: \", sentence2)\n",
    "print(\"Word:  \", word2)\n",
    "print(\"Sense: \" ,sense2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\llm\\.venv\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk ,adapted_lesk,cosine_lesk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(r'C:\\llm\\.venv\\Lib\\site-packages\\pywsd')\n",
    "nltk.data.path.append(r\"C:\\llm\\.venv\\Lib\\site-packages\\nltk\\corpora\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     .venv\\Lib\\site-packages\\nltk...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=\".venv\\Lib\\site-packages\\\\nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example sentence\n",
    "from pywsd.lesk import simple_lesk ,adapted_lesk,cosine_lesk\n",
    "sentences= [\n",
    "    \"I went to the bank to deposit money.\",\n",
    "    \"The river bank was flooded after the heavy rain.\"\n",
    "]\n",
    "\n",
    "word=\"bank\"\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    print(\"Sentence: \",sentence)\n",
    "    sense_simple = simple_lesk(sentence,word)\n",
    "    print(\"Sense simple: \", sense_simple.definiton())\n",
    "    \n",
    "    sense_adapted =adapted_lesk(sentence,word)\n",
    "    print(\"Sense Adapted: \", sense_adapted.definiton())\n",
    "    \n",
    "    \n",
    "    \n",
    "    sense_cosine =cosine_lesk(sentence,word)\n",
    "    print(\"Sensecosine: \", sense_cosine.definiton())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
