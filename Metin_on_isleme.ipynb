{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metin Ön İşleme Adımları\n",
    "- Veri Temizleme \n",
    "- Tokenizasyon \n",
    "- Kök ve Gövde Analizi (Stemming ve Lemmatization)\n",
    "- Durdurma Kelimeleri (Stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Temizleme Adımları \n",
    "- Boşlukların temizlenmesi\n",
    "- Büyük küçük harf dönüşümleri\n",
    "- Noktalama işaretlerinin kaldırılması \n",
    "- Özel karakterlerin kaldırılması\n",
    "- Yazım Hatalarının düzeltilmesi \n",
    "- Html ve URL temizleme\n",
    "### Model performansını olumsuz etkileyebilcek öğeler kaldırılır veya düzeltilir..Daha güvenilir analizler için veri temizleme yaparız"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install textblob  spacy gensim  transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metinlerdeki fazla boslukları temizle\n",
    "text =\"Hello,     World!    2035\"#\"Hello,World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'World!', '2035']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=\" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! 2035\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Büyükten küçüğe harf çevrimi\n",
    "text =\"Hello,World 2035!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,world 2035!\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noktalama işaretlerini kaldır\n",
    "text=\"Hello,World! 2035\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "cleaned_text2= text.translate(str.maketrans(\"\",\"\",string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld 2035\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello, World! 2035\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Özel karakterleri kaldır\n",
    "import re \n",
    "cleaned_text4= re.sub(r\"[^A-Za-z0-9\\s]\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World 2035\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yazım hatalarının düzeltilmesi\n",
    "\n",
    "from textblob import TextBlob\n",
    "text = \"HeLıo, Wirld!  2035\"\n",
    "cleaned_text5= str(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeLıo, World!  2035\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizasyon\n",
    "- Bir metni daha küçük parçalara ayırma islemi \n",
    "- Kelimelere ,cümlelere ,hatta karakterlere ayırılabilir\n",
    "- Token bu parçalara denir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\llm\\.venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", download_dir=\".venv\\Lib\\site-packages\\\\nltk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to .venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab', download_dir=\".venv\\Lib\\site-packages\\\\nltk\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"Hello, World! How are you? Hi ...\"\n",
    "\n",
    "# kelimeleri tokenlara ayir\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# cumle tokenization\n",
    "sentence_tokens = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!', 'How', 'are', 'you', '?', 'Hi', '...']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kök ve Gövde Analizi \n",
    "- Stemming(kök bulma) temel anlamını bulmak -suffix çıkarma yaparız .Temel amac  kelimenin anlamını tamamen doğru bulmayı amaçlamaz-kelime köküne bakar \n",
    "    - Örnek : \n",
    "        - koşuyor,koştu,koşmak : koş\n",
    "\n",
    "- Lemmatization(gövdeleme) kelimeleri sözlükteki temel formlarına dönüştürme denir \n",
    "    - Örnek\n",
    "        - koşuyor,koştu,koşmak : koşmak \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to .venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\", download_dir=\".venv\\Lib\\site-packages\\\\nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"ran\", \"runs\", \"better\", \"go\", \"went\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem result:  ['run', 'runner', 'ran', 'run', 'better', 'go', 'went']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stem result: \", stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma result  :  ['run', 'runner', 'run', 'run', 'better', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"runs\", \"better\", \"go\", \"went\"]\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(w,pos=\"v\")for w in words]\n",
    "print(\"Lemma result  : \",lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "- Metin analizi sırasında bir işe yaramayan(faydalı olmayan) kelimeleri ayıkladığımız süreç\n",
    "- exp:Edat ,zamirler,zamirler\n",
    "- Modelin verimliliğini arttırmak için gereklidir.\n",
    "- Örnek tr: ve,bir,ile,da,de,mi\n",
    "        eng:and,the,is,to,of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to .venv\\Lib\\site-\n",
      "[nltk_data]     packages\\nltk...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\", download_dir=\".venv\\Lib\\site-packages\\\\nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word liste yukle\n",
    "nltk.data.path = [r'c:\\\\llm\\.venv\\Lib\\site-packages\\nltk']\n",
    "stop_words_eng = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_words:  ['example', 'removing', 'stop', 'words', 'text', 'document.']\n",
      "filtered_words:  ['merhaba', 'dünya', 'güzel', 'insanlar']\n",
      "filtered_words:  ['örnek', 'metin', 'stop', \"words'leri\", 'temizlemek', 'kullanılıyor.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ornek metin\n",
    "text = \"This is an example of removing stop words from a text document.\"\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words_eng]\n",
    "print(\"filtered_words: \",filtered_words)\n",
    "\n",
    "# stop word liste yukle\n",
    "stop_words_tr = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "# ornek metin turkce\n",
    "text = \"merhaba dünya ve bu güzel insanlar\"\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words_tr]\n",
    "print(\"filtered_words: \",filtered_words)\n",
    "\n",
    "# %%\n",
    "\n",
    "turkish_stopwords = set([\"ve\", \"bir\", \"bu\", \"ile\", \"için\"])\n",
    "\n",
    "# ornek metin\n",
    "text = \"Bu bir örnek metin ve stop words'leri temizlemek için kullanılıyor.\"\n",
    "filtered_words = [word for word in text.split() if word.lower() not in turkish_stopwords]\n",
    "print(\"filtered_words: \",filtered_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
